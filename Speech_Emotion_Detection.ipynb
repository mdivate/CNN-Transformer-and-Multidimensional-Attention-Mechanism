{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSOxuK3u6pGa7KXQaWhi6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdivate/CNN-Transformer-and-Multidimensional-Attention-Mechanism/blob/main/Speech_Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 --index-url https://download.pytorch.org/whl/cu126"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sin7LZM2Wm5K",
        "outputId": "ef759cb9-5793-4030-a8f2-971b4506591d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.9.0+cu126\n",
            "Uninstalling torch-2.9.0+cu126:\n",
            "  Successfully uninstalled torch-2.9.0+cu126\n",
            "Found existing installation: torchvision 0.24.0+cu126\n",
            "Uninstalling torchvision-0.24.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.24.0+cu126\n",
            "Found existing installation: torchaudio 2.9.0+cu126\n",
            "Uninstalling torchaudio-2.9.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Collecting torch==2.9.0\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torch-2.9.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision==0.24.0\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.24.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting torchaudio==2.9.0\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.9.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.24.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.24.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0) (3.0.3)\n",
            "Downloading https://download.pytorch.org/whl/cu126/torch-2.9.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (832.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.9/832.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchvision-0.24.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (7.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchaudio-2.9.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-2.9.0+cu126 torchaudio-2.9.0+cu126 torchvision-0.24.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJSTloi7XoGN",
        "outputId": "498d5937-cf41-475e-9eb9-ed39a4f90622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "CUDA: True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q librosa soundfile scikit-learn pandas\n",
        "!pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D168gaeyZEYU",
        "outputId": "d79df1bb-f3e4-40c8-d5be-deff3d0f4dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.24.0 in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.9.0 in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.24.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.24.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0) (3.0.3)\n",
            "Torch: 2.9.0+cu126\n",
            "CUDA: True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwKSM4VZZJWy",
        "outputId": "76d5be29-eb08-415b-dfd9-c89733c818a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KMSsDYdZM7z",
        "outputId": "9d7009fb-5411-4404-d0ef-a4790e55228b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IEMOCAP_PATH = \"/content/drive/MyDrive/IEMOCAP_full_release/\"\n",
        "EMO_LABELS = {\"ang\":0, \"hap\":1, \"sad\":2, \"neu\":3}\n"
      ],
      "metadata": {
        "id": "GjL1YSeVZbN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "\n",
        "iemocap_path = \"/content/drive/MyDrive/IEMOCAP_full_release\"\n",
        "emotion_map = {}   # filename → emotion\n",
        "\n",
        "# Regex matches exactly the format you pasted\n",
        "line_regex = re.compile(\n",
        "    r'^\\[(.*?)\\]\\s+([A-Za-z0-9_]+)\\s+([a-z]{3})\\s+\\[(.*?)\\]'\n",
        ")\n",
        "\n",
        "label_files = glob.glob(iemocap_path + \"/**/dialog/EmoEvaluation/*.txt\", recursive=True)\n",
        "print(\"Label files found:\", len(label_files))\n",
        "\n",
        "for lf in label_files:\n",
        "    with open(lf, \"r\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            match = line_regex.match(line)\n",
        "            if match:\n",
        "                filename = match.group(2) + \".wav\"\n",
        "                emotion = match.group(3).lower()\n",
        "\n",
        "                # skip xxx, not labeled emotions\n",
        "                if emotion == \"xxx\":\n",
        "                    continue\n",
        "\n",
        "                emotion_map[filename] = emotion\n",
        "\n",
        "print(\"Total Labeled Files:\", len(emotion_map))\n",
        "print(list(emotion_map.items())[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cags3FxzZg2a",
        "outputId": "1ceea924-ad95-429d-8509-07d77023ea62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label files found: 28\n",
            "Total Labeled Files: 1404\n",
            "[('Ses01F_script01_2_F003.wav', 'ang'), ('Ses01F_script01_2_F004.wav', 'ang'), ('Ses01F_script01_2_F005.wav', 'ang'), ('Ses01F_script01_2_F006.wav', 'ang'), ('Ses01F_script01_2_F007.wav', 'ang'), ('Ses01F_script01_2_F008.wav', 'ang'), ('Ses01F_script01_2_F009.wav', 'ang'), ('Ses01F_script01_2_F010.wav', 'ang'), ('Ses01F_script01_2_F011.wav', 'ang'), ('Ses01F_script01_2_F012.wav', 'ang')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive/IEMOCAP_full_release/\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7-JDur6a4RU",
        "outputId": "f682f325-497b-4a0a-d779-5d9b80a40325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Session1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wav_files = glob.glob(IEMOCAP_PATH + \"**/*.wav\", recursive=True)\n",
        "print(\"WAV files found:\", len(wav_files))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsF36uWEb4UC",
        "outputId": "5851333e-3b41-4fca-81a0-09501f4be4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WAV files found: 114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for wav in wav_files:\n",
        "    key = wav.split(\"/\")[-1]\n",
        "    if key in emotion_map:\n",
        "        label_str = emotion_map[key]\n",
        "        label_idx = EMO_LABELS.index(label_str)  # get integer index\n",
        "        data.append((wav, label_idx))\n",
        "\n",
        "print(\"Final usable dataset:\", len(data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2j5TjQob90L",
        "outputId": "8a9e7f14-02f2-404c-a036-6e0b288a4fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final usable dataset: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torchaudio\n",
        "!pip install torchaudio==2.8.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD61VUYKd1TU",
        "outputId": "0c303493-d3cf-4d6c-ee02-dffb003c571b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio==2.8.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.9.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio==2.8.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "iemocap_path = \"/content/drive/MyDrive/IEMOCAP_full_release/\"\n",
        "\n",
        "# Use the emotions we want\n",
        "EMO_LABELS = [\"ang\", \"hap\", \"sad\", \"neu\", \"exc\"]\n",
        "\n",
        "emotion_map = {}\n",
        "\n",
        "label_files = glob.glob(os.path.join(iemocap_path, \"*/dialog/EmoEvaluation/*.txt\"))\n",
        "\n",
        "for lf in label_files:\n",
        "    with open(lf, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        current_wav = None\n",
        "        current_emotions = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"[\") and \"\\t\" in line:\n",
        "                # New segment\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 2:\n",
        "                    current_wav = parts[1]  # e.g., Ses01F_script01_2_F000\n",
        "                    current_emotions = []\n",
        "            elif line.startswith(\"C-\") and \":\" in line and current_wav:\n",
        "                # Extract annotator emotion\n",
        "                emo = line.split(\":\")[1].strip().lower().rstrip(\";\")\n",
        "                # Map IEMOCAP labels to simple labels\n",
        "                if emo.startswith(\"anger\"):\n",
        "                    emo_simple = \"ang\"\n",
        "                elif emo.startswith(\"happiness\") or emo.startswith(\"excited\"):\n",
        "                    emo_simple = \"hap\"\n",
        "                elif emo.startswith(\"sadness\"):\n",
        "                    emo_simple = \"sad\"\n",
        "                elif emo.startswith(\"neutral\"):\n",
        "                    emo_simple = \"neu\"\n",
        "                elif emo.startswith(\"excited\"):\n",
        "                    emo_simple = \"exc\"\n",
        "                else:\n",
        "                    emo_simple = None\n",
        "\n",
        "                if emo_simple in EMO_LABELS:\n",
        "                    current_emotions.append(emo_simple)\n",
        "            elif line == \"\" and current_wav:\n",
        "                # End of segment, take first emotion if exists\n",
        "                if current_emotions:\n",
        "                    emotion_map[current_wav] = current_emotions[0]\n",
        "                current_wav = None\n"
      ],
      "metadata": {
        "id": "5eQmaVCPcmla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wav_files = glob.glob(os.path.join(iemocap_path, \"**/*.wav\"), recursive=True)\n",
        "data = []\n",
        "\n",
        "for wav in wav_files:\n",
        "    key = os.path.basename(wav).replace(\".wav\",\"\")\n",
        "    if key in emotion_map:\n",
        "        data.append((wav, emotion_map[key]))\n",
        "\n",
        "print(\"Final usable dataset:\", len(data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6tFVNZze266",
        "outputId": "0f9020d0-8da6-4da7-c478-07aef6fa77d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final usable dataset: 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgcEp73VfzNU",
        "outputId": "2a0a2816-e69d-47ca-90c4-57178352fe7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.9.0+cu126\n",
            "Uninstalling torch-2.9.0+cu126:\n",
            "  Successfully uninstalled torch-2.9.0+cu126\n",
            "\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.0+cu118 torchaudio==2.2.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlSPoYMSfzcL",
        "outputId": "da0cec1d-b12c-4d70-c8ed-3bc7f1fb5fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch==2.2.0+cu118 in /usr/local/lib/python3.12/dist-packages (2.2.0+cu118)\n",
            "Requirement already satisfied: torchaudio==2.2.0+cu118 in /usr/local/lib/python3.12/dist-packages (2.2.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (11.8.86)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0+cu118) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0+cu118) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "print(torchaudio.get_audio_backend())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4h35Frlgb3z",
        "outputId": "4100c029-4906-478a-dc40-f2782a74baa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-3249698272.py\", line 1, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 1471, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0+cu118\n",
            "2.2.0+cu118\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3249698272.py:6: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  print(torchaudio.get_audio_backend())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "# Path to your IEMOCAP dataset\n",
        "iemocap_path = \"/content/drive/MyDrive/IEMOCAP_full_release/\"\n",
        "\n",
        "# Emotions we care about\n",
        "EMO_LABELS = [\"ang\", \"hap\", \"sad\", \"neu\", \"exc\"]\n",
        "\n",
        "# Build emotion map from EmoEvaluation files\n",
        "emotion_map = {}\n",
        "label_files = glob.glob(os.path.join(iemocap_path, \"*/dialog/EmoEvaluation/*.txt\"))\n",
        "\n",
        "for lf in label_files:\n",
        "    with open(lf, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        current_wav = None\n",
        "        current_emotions = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"[\") and \"\\t\" in line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 2:\n",
        "                    current_wav = parts[1]  # file ID\n",
        "                    current_emotions = []\n",
        "            elif line.startswith(\"C-\") and \":\" in line and current_wav:\n",
        "                emo = line.split(\":\")[1].strip().lower().rstrip(\";\")\n",
        "                if emo.startswith(\"anger\"):\n",
        "                    emo_simple = \"ang\"\n",
        "                elif emo.startswith(\"happiness\") or emo.startswith(\"excited\"):\n",
        "                    emo_simple = \"hap\"\n",
        "                elif emo.startswith(\"sadness\"):\n",
        "                    emo_simple = \"sad\"\n",
        "                elif emo.startswith(\"neutral\"):\n",
        "                    emo_simple = \"neu\"\n",
        "                elif emo.startswith(\"excited\"):\n",
        "                    emo_simple = \"exc\"\n",
        "                else:\n",
        "                    emo_simple = None\n",
        "                if emo_simple in EMO_LABELS:\n",
        "                    current_emotions.append(emo_simple)\n",
        "            elif line == \"\" and current_wav:\n",
        "                if current_emotions:\n",
        "                    emotion_map[current_wav] = current_emotions[0]\n",
        "                current_wav = None\n",
        "\n",
        "# Match WAV files to labels\n",
        "wav_files = glob.glob(os.path.join(iemocap_path, \"**/*.wav\"), recursive=True)\n",
        "data = []\n",
        "for wav in wav_files:\n",
        "    key = os.path.basename(wav).replace(\".wav\",\"\")\n",
        "    if key in emotion_map:\n",
        "        data.append((wav, emotion_map[key]))\n",
        "\n",
        "print(\"Final usable dataset:\", len(data))\n",
        "print(\"First 5 entries:\", data[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBTEZRUafzmL",
        "outputId": "a2129c40-dae1-4392-ffdd-b87ff2bc2900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final usable dataset: 85\n",
            "First 5 entries: [('/content/drive/MyDrive/IEMOCAP_full_release/Session1/sentences/wav/Ses01M_script03_2_F013.wav', 'ang'), ('/content/drive/MyDrive/IEMOCAP_full_release/Session1/sentences/wav/Ses01M_script03_2_F033.wav', 'ang'), ('/content/drive/MyDrive/IEMOCAP_full_release/Session1/sentences/wav/Ses01M_script03_2_F025.wav', 'ang'), ('/content/drive/MyDrive/IEMOCAP_full_release/Session1/sentences/wav/Ses01M_script03_2_F019.wav', 'ang'), ('/content/drive/MyDrive/IEMOCAP_full_release/Session1/sentences/wav/Ses01M_script03_2_F037.wav', 'ang')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "\n",
        "# Check available backends\n",
        "print(\"Available torchaudio backends:\", torchaudio.list_audio_backends())\n",
        "\n",
        "# Set the backend explicitly to soundfile\n",
        "torchaudio.set_audio_backend(\"soundfile\")\n",
        "\n",
        "# Load the first WAV\n",
        "wav_path = data[0][0]\n",
        "waveform, sample_rate = torchaudio.load(wav_path)\n",
        "print(\"Waveform shape:\", waveform.shape)\n",
        "print(\"Sample rate:\", sample_rate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCWOnRdXhLJU",
        "outputId": "637b9142-c59a-4d38-ed92-c76a59db7124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available torchaudio backends: ['ffmpeg', 'soundfile']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3584490978.py:7: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waveform shape: torch.Size([1, 97600])\n",
            "Sample rate: 16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 1️⃣ Imports\n",
        "# -----------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# -----------------------------\n",
        "# 2️⃣ Use the data list you already created\n",
        "# data = [(wav_path, label_str), ...] with 85 samples\n",
        "# -----------------------------\n",
        "labels = [label for _, label in data]\n",
        "le = LabelEncoder()\n",
        "le.fit(labels)\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "# -----------------------------\n",
        "# 3️⃣ PyTorch Dataset\n",
        "# -----------------------------\n",
        "class SERDataset(Dataset):\n",
        "    def __init__(self, data, label_encoder, n_mfcc=40, max_len=200):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: list of tuples (wav_path, label_str)\n",
        "            label_encoder: sklearn LabelEncoder fitted on labels\n",
        "            n_mfcc: number of MFCC features\n",
        "            max_len: maximum number of time steps (for padding/truncation)\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.le = label_encoder\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav_path, label_str = self.data[idx]\n",
        "\n",
        "        # Load waveform using soundfile backend\n",
        "        waveform, sample_rate = torchaudio.load(wav_path)\n",
        "\n",
        "        # Convert to mono if needed\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Extract MFCC features\n",
        "        mfcc = torchaudio.transforms.MFCC(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mfcc=self.n_mfcc,\n",
        "            melkwargs={'n_mels': 40, 'n_fft': 400, 'hop_length': 160}\n",
        "        )(waveform)  # shape: [1, n_mfcc, time]\n",
        "\n",
        "        mfcc = mfcc.squeeze(0).T  # shape: [time, n_mfcc]\n",
        "\n",
        "        # Pad or truncate to fixed length\n",
        "        if mfcc.shape[0] < self.max_len:\n",
        "            pad = torch.zeros(self.max_len - mfcc.shape[0], self.n_mfcc)\n",
        "            mfcc = torch.cat([mfcc, pad], dim=0)\n",
        "        else:\n",
        "            mfcc = mfcc[:self.max_len, :]\n",
        "\n",
        "        # Convert label to tensor\n",
        "        label = torch.tensor(self.le.transform([label_str])[0], dtype=torch.long)\n",
        "\n",
        "        return mfcc.float(), label\n",
        "# -----------------------------\n",
        "# 4️⃣ Train/Test Split\n",
        "# -----------------------------\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "train_dataset = SERDataset(train_data, le)\n",
        "test_dataset = SERDataset(test_data, le)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 5️⃣ CNN-BiLSTM Model\n",
        "# -----------------------------\n",
        "class CNN_BiLSTM(nn.Module):\n",
        "    def __init__(self, n_mfcc=40, hidden_dim=128, num_layers=1, num_classes=num_classes):\n",
        "        super(CNN_BiLSTM, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(n_mfcc, 64, kernel_size=5, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_dim, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # [batch, n_mfcc, time]\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = x.permute(0, 2, 1)  # [batch, time, features]\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1, :])  # take last timestep\n",
        "        return out\n",
        "\n",
        "# -----------------------------\n",
        "# 6️⃣ Training Setup\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN_BiLSTM().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 20\n",
        "\n",
        "# -----------------------------\n",
        "# 7️⃣ Training Loop\n",
        "# -----------------------------\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for mfccs, labels_batch in train_loader:\n",
        "        mfccs, labels_batch = mfccs.to(device), labels_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(mfccs)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * mfccs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels_batch.size(0)\n",
        "        correct += (predicted == labels_batch).sum().item()\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} - Acc: {train_acc:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8️⃣ Evaluation\n",
        "# -----------------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for mfccs, labels_batch in test_loader:\n",
        "        mfccs, labels_batch = mfccs.to(device), labels_batch.to(device)\n",
        "        outputs = model(mfccs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels_batch.size(0)\n",
        "        correct += (predicted == labels_batch).sum().item()\n",
        "test_acc = correct / total\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2h7FpxvfOl8",
        "outputId": "1bc0ad61-3443-4db4-bcfe-17678dcfef84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss: 0.8705 - Acc: 0.6618\n",
            "Epoch 2/20 - Loss: 0.7447 - Acc: 0.7353\n",
            "Epoch 3/20 - Loss: 0.6769 - Acc: 0.7353\n",
            "Epoch 4/20 - Loss: 0.6584 - Acc: 0.7500\n",
            "Epoch 5/20 - Loss: 0.5146 - Acc: 0.7941\n",
            "Epoch 6/20 - Loss: 0.4596 - Acc: 0.8529\n",
            "Epoch 7/20 - Loss: 0.3729 - Acc: 0.8824\n",
            "Epoch 8/20 - Loss: 0.3109 - Acc: 0.8824\n",
            "Epoch 9/20 - Loss: 0.3347 - Acc: 0.8824\n",
            "Epoch 10/20 - Loss: 0.2706 - Acc: 0.8824\n",
            "Epoch 11/20 - Loss: 0.2758 - Acc: 0.8971\n",
            "Epoch 12/20 - Loss: 0.2437 - Acc: 0.8971\n",
            "Epoch 13/20 - Loss: 0.1752 - Acc: 0.9412\n",
            "Epoch 14/20 - Loss: 0.1724 - Acc: 0.9265\n",
            "Epoch 15/20 - Loss: 0.1208 - Acc: 0.9706\n",
            "Epoch 16/20 - Loss: 0.1021 - Acc: 0.9706\n",
            "Epoch 17/20 - Loss: 0.0779 - Acc: 0.9853\n",
            "Epoch 18/20 - Loss: 0.0738 - Acc: 0.9706\n",
            "Epoch 19/20 - Loss: 0.0891 - Acc: 0.9559\n",
            "Epoch 20/20 - Loss: 0.0656 - Acc: 0.9853\n",
            "Test Accuracy: 0.5882352941176471\n"
          ]
        }
      ]
    }
  ]
}